{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf31bb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from src.preprocessing import load_data, preprocess_data\n",
    "from src.train_model import train_and_evaluate\n",
    "\n",
    "print(\"Notebook ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55973b75",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Load Dataset ===\n",
    "df = load_data()\n",
    "display(df.head())\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c875cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Data Cleaning & Preprocessing ===\n",
    "\n",
    "# Drop columns not useful for regression\n",
    "cols_to_drop = [\"RecordID\", \"HealthImpactClass\"]\n",
    "df_clean = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "# Define features & target\n",
    "X = df_clean.drop(columns=[\"HealthImpactScore\"])\n",
    "y = df_clean[\"HealthImpactScore\"]\n",
    "\n",
    "print(\"Cleaned Data Shape:\", df_clean.shape)\n",
    "print(\"Feature Shape:\", X.shape)\n",
    "print(\"Target Shape:\", y.shape)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(\"\\nTrain:\", X_train.shape, y_train.shape)\n",
    "print(\"Test:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1433ace",
   "metadata": {},
   "source": [
    "## Baseline Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3ee41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Baseline Model Comparison ===\n",
    "\n",
    "!pip install xgboost lightgbm --quiet\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    \"ExtraTrees\": ExtraTreesRegressor(n_estimators=200, random_state=42, n_jobs=-1),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"SVR\": SVR(kernel='rbf'),\n",
    "    \"XGBoost\": XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8),\n",
    "    \"LightGBM\": LGBMRegressor(n_estimators=300, learning_rate=0.05)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Training baseline models...\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "\n",
    "    results.append([name, mae, rmse, r2])\n",
    "    print(f\"{name}: MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}\")\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"MAE\", \"RMSE\", \"R2\"])\n",
    "print(\"\\n=== Baseline Model Comparison ===\")\n",
    "display(results_df.sort_values(\"R2\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e520ea",
   "metadata": {},
   "source": [
    "## LightGBM Hyperparameter Tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34747bdf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === LightGBM Hyperparameter Tuning ===\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Running LightGBM Grid Search...\")\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "param_grid_lgbm = {\n",
    "    \"n_estimators\": [300, 500, 800],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [-1, 5, 10],\n",
    "    \"num_leaves\": [31, 50, 70],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 1.0]\n",
    "}\n",
    "\n",
    "grid_lgbm = GridSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_grid=param_grid_lgbm,\n",
    "    scoring=\"r2\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_lgbm.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Params (LightGBM):\", grid_lgbm.best_params_)\n",
    "print(\"Best CV R² (LightGBM):\", grid_lgbm.best_score_)\n",
    "\n",
    "best_lgbm = grid_lgbm.best_estimator_\n",
    "\n",
    "# Evaluate on the test set\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "pred_lgbm = best_lgbm.predict(X_test)\n",
    "\n",
    "mae_lgbm = mean_absolute_error(y_test, pred_lgbm)\n",
    "rmse_lgbm = np.sqrt(mean_squared_error(y_test, pred_lgbm))\n",
    "r2_lgbm = r2_score(y_test, pred_lgbm)\n",
    "\n",
    "print(\"\\n=== Tuned LightGBM Results ===\")\n",
    "print(f\"MAE:  {mae_lgbm:.4f}\")\n",
    "print(f\"RMSE: {rmse_lgbm:.4f}\")\n",
    "print(f\"R²:   {r2_lgbm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc19055f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === XGBoost Hyperparameter Tuning ===\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print(\"Running XGBoost Grid Search...\")\n",
    "\n",
    "xgb_model = XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "param_grid_xgb = {\n",
    "    \"n_estimators\": [300, 600, 900],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"max_depth\": [4, 6, 8],\n",
    "    \"subsample\": [0.7, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=3,\n",
    "    scoring=\"r2\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Params (XGBoost):\", grid_xgb.best_params_)\n",
    "print(\"Best CV R² (XGBoost):\", grid_xgb.best_score_)\n",
    "\n",
    "best_xgb = grid_xgb.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0625e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === Evaluate Final Tuned XGBoost Model ===\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Evaluating final tuned XGBoost model...\")\n",
    "\n",
    "# Predict\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae = mean_absolute_error(y_test, y_pred_xgb)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))\n",
    "r2 = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"\\n=== Tuned XGBoost Results ===\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de2fd5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# === LightGBM Feature Importance ===\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Use the tuned LightGBM model\n",
    "model_lgb = best_lgbm\n",
    "\n",
    "# Extract feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"importance\": model_lgb.feature_importances_\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(\"\\n=== LightGBM Feature Importance ===\")\n",
    "print(importance_df)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"LightGBM Feature Importance\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
